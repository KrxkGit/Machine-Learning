{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22000b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,Trainer,TrainingArguments\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet152, resnet50, resnet18 # 导入 resnet-152\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import pickle\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be146c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ffaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=128):\n",
    "        super(CopyTaskModel, self).__init__()\n",
    "\n",
    "        # 定义词向量，词典数为10。我们不预测两位小数。\n",
    "        self.embedding = nn.Embedding(num_embeddings=10, embedding_dim=128)\n",
    "        # 定义Transformer。超参是我拍脑袋想的\n",
    "        self.transformer = nn.Transformer(d_model=128, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512, batch_first=True)\n",
    "\n",
    "        # 定义位置编码器\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=0)\n",
    "\n",
    "        # 定义最后的线性层，这里并没有用Softmax，因为没必要。\n",
    "        # 因为后面的CrossEntropyLoss中自带了\n",
    "        self.predictor = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # 生成mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1])\n",
    "        src_key_padding_mask = CopyTaskModel.get_key_padding_mask(src)\n",
    "        tgt_key_padding_mask = CopyTaskModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        # 对src和tgt进行编码\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        # 给src和tgt的token增加位置信息\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        # 将准备好的数据送给transformer\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        这里直接返回transformer的结果。因为训练和推理时的行为不一样，\n",
    "        所以在该模型外再进行线性层的预测。\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        \"\"\"\n",
    "        用于key_padding_mask\n",
    "        \"\"\"\n",
    "        key_padding_mask = torch.zeros(tokens.size())\n",
    "        key_padding_mask[tokens == 2] = -torch.inf\n",
    "        return key_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0209031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CopyTaskModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064f6034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 128])\n",
      "tensor([[[-0.1443, -0.1420, -1.4241,  1.3634,  0.9349, -1.4224, -1.0949,\n",
      "          -0.1085, -1.0228,  1.3970,  0.7386, -1.4319, -0.7540, -0.0611,\n",
      "          -0.1000, -0.5736, -1.0333, -1.2741, -1.8638,  0.0470,  0.8020,\n",
      "           1.5381,  0.2843, -1.2397, -0.5574, -0.0993, -0.2226,  1.0915,\n",
      "          -0.6805,  1.2655,  0.8416,  0.6378, -0.0479,  1.4101, -0.4453,\n",
      "           0.2572,  1.0721,  0.3767, -0.5386,  0.4034, -2.0918, -0.8804,\n",
      "          -1.1549,  0.7348,  0.0442,  0.2122, -0.9396, -0.2617,  0.2342,\n",
      "           0.8461, -0.1015, -0.4978,  0.1787, -1.2960, -1.0765,  1.4962,\n",
      "           0.6281, -0.8321, -0.5948, -0.0889,  1.0555,  0.2710,  0.0245,\n",
      "           0.7625, -0.4059, -0.3218,  0.1276,  1.6868,  0.1329,  2.0615,\n",
      "          -1.0582,  1.0689,  1.6706,  0.7455,  0.7841, -0.6914,  0.4980,\n",
      "           0.2294, -0.1563, -0.5104,  1.4300, -1.3512, -1.7642,  1.2600,\n",
      "          -0.3690, -0.4674, -0.5708, -0.8825,  0.2682,  0.9221, -1.1555,\n",
      "          -0.2634,  2.4917,  0.6976,  0.4060, -0.6837, -0.1231, -0.8763,\n",
      "           0.6875, -1.9814, -0.5465, -0.2998, -0.7702,  1.0926, -1.8231,\n",
      "          -0.9638, -0.8852,  1.4678, -0.2558, -1.2500, -2.9187,  0.2891,\n",
      "           1.7372,  0.2470, -0.5251,  1.3586, -0.5320,  0.9951,  0.0102,\n",
      "          -0.5461,  0.9065,  0.4127, -0.8830, -0.3260,  1.8836,  0.9838,\n",
      "           0.9074,  1.8464],\n",
      "         [ 0.9631,  0.8931, -1.1557, -0.5657,  0.0496, -0.8158, -0.4963,\n",
      "           0.9158, -0.2803, -0.4988,  1.6834, -0.0150, -2.7557,  0.4625,\n",
      "          -0.3082, -0.7389, -0.5767,  0.3622, -1.6901, -0.2859,  0.3998,\n",
      "           0.8411,  1.9184,  0.0834,  1.6728,  0.4821, -0.7225,  0.5475,\n",
      "           0.3438,  0.5617,  0.7406,  0.4939, -0.6453,  1.2817, -0.0704,\n",
      "           0.4146,  0.0154,  0.6193,  0.0260,  1.3881, -1.4697, -0.0873,\n",
      "          -1.3737,  0.1206, -0.2586, -0.0916, -1.1049, -0.6385,  0.3771,\n",
      "           0.2764, -0.6821, -1.2934,  0.3137,  0.0278, -1.4886,  1.3876,\n",
      "          -1.1163,  0.1448, -0.9616, -0.9792,  0.1834,  0.4774, -1.1388,\n",
      "           1.4172,  0.3259, -0.6955,  0.3505,  0.6903, -0.9461,  1.6793,\n",
      "          -1.1213,  1.2419,  0.7311,  0.3897, -0.0281, -1.5068,  1.1369,\n",
      "           0.1204,  0.7535, -0.4397,  0.8692, -0.4247,  0.0493,  1.0597,\n",
      "          -0.3459, -0.2919, -1.3083, -2.2398,  0.2540,  0.4462, -0.8566,\n",
      "          -0.0496,  2.0747, -1.0584,  1.8498,  1.1825,  0.6977, -0.8652,\n",
      "           0.1919,  0.3897, -1.3197, -0.3414, -0.9461,  0.7495, -2.1899,\n",
      "           0.7700, -0.8098,  1.0230,  0.0480, -2.3469, -2.2687,  1.7724,\n",
      "           0.0333, -0.1728, -1.4436,  0.9479,  0.4229,  0.7286,  0.9044,\n",
      "          -0.3701,  0.5295, -0.6953, -1.3575,  0.2862, -0.7298,  1.6946,\n",
      "           1.3600,  1.8349],\n",
      "         [ 0.5837, -0.2654, -0.9909, -0.3713,  0.6032, -1.9891, -0.1777,\n",
      "           0.1365,  0.2878,  0.1259,  0.8530, -1.1280, -2.3707, -0.3228,\n",
      "          -0.7318,  1.1430, -2.4146,  0.0772, -1.8965,  0.4500,  2.0454,\n",
      "           0.9265, -0.2493, -1.0337, -0.2073, -0.7413,  0.1127,  0.4829,\n",
      "          -0.5547,  0.5800,  0.8072,  0.9002, -0.6384,  1.3936, -0.5567,\n",
      "           1.1872, -0.7613,  0.5305,  0.6286,  1.7440, -1.6128,  0.0185,\n",
      "          -0.3625,  0.3816,  0.1417,  0.2124, -1.4167, -0.5187, -1.8034,\n",
      "           1.4595,  0.1386, -0.5657, -0.1208, -0.2515, -1.7359,  2.2927,\n",
      "          -0.3407, -0.1071, -0.2419,  0.5674, -0.6497,  0.6870,  0.2810,\n",
      "          -0.1612, -1.0625,  1.4853, -0.7476,  1.9459, -0.3243,  1.1114,\n",
      "           0.4428,  1.1463,  0.7646,  0.4780,  0.8075, -0.9673,  0.6086,\n",
      "           0.5190,  0.0902,  0.0877,  0.3932, -1.5513, -0.5055,  0.2163,\n",
      "           0.2202, -0.4549, -1.7916, -0.8766,  0.2917,  1.9654,  0.4820,\n",
      "          -0.6479,  2.8820, -0.7280,  0.2383,  0.7743, -0.1560, -0.8082,\n",
      "           0.1793, -0.3373, -1.1498, -0.2426, -1.3233,  0.2538, -2.1792,\n",
      "           0.8719, -1.4607,  0.4564,  1.2682, -2.1228, -0.9393,  0.6776,\n",
      "           1.1954,  0.1485, -0.3069,  1.2846,  0.4409,  1.0271,  0.5025,\n",
      "          -1.2604,  0.1339,  0.0872, -0.7757, -0.5868,  0.6635,  0.6826,\n",
      "           0.6328,  1.3602],\n",
      "         [ 0.4013, -1.0830, -1.6765, -0.5946, -0.8652, -1.5349, -0.4640,\n",
      "           1.3954,  1.0940,  0.5314,  1.7554, -2.0081, -1.0006,  0.3275,\n",
      "          -0.5169, -0.2979, -1.2684, -0.7410,  0.1230,  0.5353,  1.0500,\n",
      "           1.4068,  0.7414, -1.1695,  0.4434,  0.2995, -0.2581,  0.4720,\n",
      "          -0.6023, -0.0272,  0.3196, -1.1395,  0.3772,  1.5851, -0.1819,\n",
      "           0.4656,  0.3270, -0.1817,  1.2901,  1.6940, -1.4586,  0.6480,\n",
      "          -1.4210, -0.1451, -0.2783, -0.1770,  0.5346, -0.2756, -0.5471,\n",
      "           1.0926,  0.1596, -0.7705, -0.5360,  1.7258, -1.3958,  0.9865,\n",
      "          -1.0523,  0.2172, -0.8704,  1.1393, -0.4864,  1.7151,  0.0535,\n",
      "           1.8221,  0.0094, -0.3641, -0.1360,  0.8198, -1.1915,  0.5979,\n",
      "           0.3287,  0.7121,  1.0026,  1.0619, -0.0808, -1.5325,  1.5450,\n",
      "          -0.8461,  1.3337,  0.1502,  0.9017,  0.1560, -0.0708,  0.4944,\n",
      "           1.1882, -0.9996, -1.1234, -0.7123,  0.0913,  2.0209, -1.7744,\n",
      "          -0.6994,  2.2714, -0.9204,  0.1521, -0.1479, -0.7653, -0.6159,\n",
      "           0.3006, -0.6938, -1.3209, -1.7582, -0.6159,  1.0274, -1.6781,\n",
      "           0.3619, -1.4277,  0.8354,  0.0203, -2.5626, -2.1212,  0.9784,\n",
      "           0.4045, -0.4256, -0.6361,  0.4311, -0.4835,  0.3757,  0.3144,\n",
      "          -0.9531,  0.2873,  0.0880, -0.3410,  0.1687,  0.4921,  0.8769,\n",
      "           1.0787,  2.4150],\n",
      "         [ 0.9192,  0.4423, -1.2101, -0.9949,  0.1076, -1.9056, -0.1108,\n",
      "          -0.9368,  1.0455, -0.0768,  0.6115, -2.0153, -2.3347, -0.3182,\n",
      "           0.7593, -1.3004, -1.1346, -0.4805, -0.8928, -1.3275,  2.0265,\n",
      "           1.5627,  0.7113,  0.3654,  0.4176,  0.1512, -0.8819, -0.4850,\n",
      "          -0.6128,  1.0679,  0.2559,  0.3102,  0.4312,  0.9059, -0.6447,\n",
      "           0.9920,  0.8801,  0.0223,  0.5721,  1.0544, -1.3935,  0.5381,\n",
      "          -0.1675, -0.1565, -0.3849,  0.6078, -0.6046,  0.0249,  0.3212,\n",
      "           0.5630, -0.2885, -0.8399, -0.8298,  1.1682, -0.3514,  1.8731,\n",
      "           0.3341, -0.6153, -1.2947,  0.2249, -0.0562,  1.1437,  0.7921,\n",
      "           1.0685, -1.0617, -0.5105, -0.0186, -0.4679, -1.0981,  0.3751,\n",
      "           1.2612,  1.2754,  1.7832,  0.9876, -0.2245, -0.7635,  0.3224,\n",
      "           0.3318,  1.0292,  0.2278, -0.0996,  0.0895, -0.7033, -0.2275,\n",
      "           0.1557,  1.5361, -1.9248, -0.9191,  0.4949,  1.1676, -1.0192,\n",
      "           0.4621,  1.0002, -0.8711,  0.7495,  0.5726,  0.6393, -0.4125,\n",
      "          -0.4782,  0.2181, -1.7253, -0.7511, -0.1690,  0.6127, -3.1851,\n",
      "          -0.2309, -1.9441,  1.3410,  1.5505, -2.4507, -2.4483,  0.3595,\n",
      "           1.4321,  0.2119, -0.3097, -0.8019,  0.4829,  0.1151,  0.3837,\n",
      "          -0.0659,  1.0267, -0.6526, -0.1867, -0.1532, -0.0334,  1.3667,\n",
      "           0.4269,  2.2936],\n",
      "         [-0.5410, -0.0829, -0.8545, -0.2419,  0.0104, -2.3336, -0.7752,\n",
      "          -0.0883,  0.6426,  0.8598,  0.8691, -0.6836, -1.9550, -0.2910,\n",
      "           0.3172, -0.8706, -0.9853, -1.3934, -1.3520, -0.6089,  2.0424,\n",
      "           0.6755, -0.6668, -0.9761,  0.8088,  0.1991,  0.1503, -0.1264,\n",
      "          -0.2292, -0.2696,  0.4242, -0.3698,  1.4767,  0.3557,  0.0611,\n",
      "           1.0662,  1.0543,  0.1771,  1.5809,  2.7630, -1.5743, -0.7556,\n",
      "           0.1311, -0.8891, -0.9140,  0.7948,  0.0682, -1.1087, -1.3917,\n",
      "           1.1058,  0.9639, -1.0793, -0.0100,  1.1737, -0.5579,  1.6137,\n",
      "          -1.4245, -0.5520, -0.7019,  1.2063,  0.5056,  1.5882, -0.5179,\n",
      "           1.6950, -0.1265,  0.4966,  0.1175,  0.9472, -0.5440,  1.6455,\n",
      "           0.2431,  1.4146,  0.9142,  0.8272, -0.4386, -0.7135, -0.0066,\n",
      "           1.3548,  0.9257, -0.7291,  0.1391,  0.5009, -0.0236,  0.3486,\n",
      "           0.3361, -0.1928, -0.3045, -2.1725,  0.1339,  1.6857, -0.5602,\n",
      "           0.1800,  0.8199, -0.5175, -0.2671,  1.0924, -0.5796,  0.1262,\n",
      "          -0.7272, -0.9227, -0.6289, -0.7534,  0.0057, -0.4553, -2.9194,\n",
      "          -0.3711, -1.0629,  0.5125, -0.0772, -2.3583, -2.6702,  0.6226,\n",
      "           0.9076,  0.1950, -0.7091,  1.0160, -0.1507, -0.7170,  0.1074,\n",
      "          -0.3618,  1.0134, -0.0450,  0.5390, -0.1303,  0.6381,  0.9450,\n",
      "          -0.2000,  2.4767],\n",
      "         [ 0.2062,  0.7308, -1.2259,  0.4332, -0.5156, -1.7277, -1.1480,\n",
      "           0.1385,  0.0254,  0.8034,  0.7549, -1.3174, -2.1475, -0.3287,\n",
      "           0.4419, -0.5923, -1.7013,  0.0803,  0.6675, -0.8096,  0.9945,\n",
      "           0.2028, -0.3460, -1.2591,  1.0552,  0.1903,  0.0077, -0.4852,\n",
      "          -0.5499,  0.0982, -0.1444, -0.7758,  0.3007,  1.6212,  0.0720,\n",
      "           0.8958,  1.2294,  0.3047,  0.6427,  1.9150, -0.4160, -1.1910,\n",
      "          -0.0985, -1.0646, -1.1244,  0.3492,  0.1681, -0.9660, -1.2315,\n",
      "           1.6288,  0.2267, -0.6843, -0.9132,  1.7800,  0.2887,  1.7296,\n",
      "          -1.0583, -0.2532, -0.8523,  1.5777,  0.1591,  0.9071, -0.9321,\n",
      "           1.6987, -0.8795, -0.5143, -0.4981,  1.3184, -0.3595,  0.8171,\n",
      "           0.6124,  1.6056,  1.2369,  0.4055,  0.5569, -1.2697, -0.1789,\n",
      "          -0.1095,  0.5552, -1.2508,  0.3258, -0.2382, -1.0624,  1.8223,\n",
      "           0.5401, -0.3830, -1.4399, -1.2799,  0.2685,  1.3190, -0.5979,\n",
      "          -0.7435,  1.4428,  0.4567, -0.4210,  0.6710, -0.3613, -0.5053,\n",
      "          -0.5935, -0.9208, -0.9243,  0.6153, -0.3683,  0.0046, -3.0691,\n",
      "          -0.5228, -0.6830,  0.8832, -0.2092, -2.1337, -2.4982,  0.3884,\n",
      "           0.8135,  0.4871,  0.0117,  0.2786, -0.0461,  0.6743,  0.1400,\n",
      "          -1.0012,  0.6236,  0.3369,  1.2792,  0.6989,  0.7895,  1.0679,\n",
      "           0.3478,  3.2013]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src = torch.LongTensor([[0, 3, 4, 5, 6, 1, 2, 2]])\n",
    "tgt = torch.LongTensor([[3, 4, 5, 6, 1, 2, 2]])\n",
    "out = model(src, tgt)\n",
    "print(out.size())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de5aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_batch(batch_size, max_length=16):\n",
    "    src = []\n",
    "    for i in range(batch_size):\n",
    "        # 随机生成句子长度\n",
    "        random_len = random.randint(1, max_length - 2)\n",
    "        # 随机生成句子词汇，并在开头和结尾增加<bos>和<eos>\n",
    "        random_nums = [0] + [random.randint(3, 9) for _ in range(random_len)] + [1]\n",
    "        # 如果句子长度不足max_length，进行填充\n",
    "        random_nums = random_nums + [2] * (max_length - random_len - 2)\n",
    "        src.append(random_nums)\n",
    "    src = torch.LongTensor(src)\n",
    "    # tgt不要最后一个token\n",
    "    tgt = src[:, :-1]\n",
    "    # tgt_y不要第一个的token\n",
    "    tgt_y = src[:, 1:]\n",
    "    # 计算tgt_y，即要预测的有效token的数量\n",
    "    n_tokens = (tgt_y != 2).sum()\n",
    "\n",
    "    # 这里的n_tokens指的是我们要预测的tgt_y中有多少有效的token，后面计算loss要用\n",
    "    return src, tgt, tgt_y, n_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2807471d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 4, 8, 9, 4, 1],\n",
       "         [0, 9, 1, 2, 2, 2]]),\n",
       " tensor([[0, 4, 8, 9, 4],\n",
       "         [0, 9, 1, 2, 2]]),\n",
       " tensor([[4, 8, 9, 4, 1],\n",
       "         [9, 1, 2, 2, 2]]),\n",
       " tensor(7))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random_batch(batch_size=2, max_length=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd4b8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9997, 1.9991, 3.0076, 3.9936], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12469\\AppData\\Local\\Temp\\ipykernel_4280\\2273729196.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  a = a - a.grad * 0.01\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4280\\2273729196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0,2,3,4],requires_grad=True)\n",
    "b = torch.tensor(2,requires_grad=False)\n",
    "for i in range(10):\n",
    "    c = F.cross_entropy(a, b)\n",
    "    c.backward()\n",
    "    a = a - a.grad * 0.01\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bba7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40, total_loss: 2.4486610889434814\n",
      "Step 80, total_loss: 2.074915885925293\n",
      "Step 120, total_loss: 1.9676580429077148\n",
      "Step 160, total_loss: 1.878318428993225\n",
      "Step 200, total_loss: 1.8532143831253052\n",
      "Step 240, total_loss: 1.676632046699524\n",
      "Step 280, total_loss: 1.4618895053863525\n",
      "Step 320, total_loss: 1.3819327354431152\n",
      "Step 360, total_loss: 1.289496660232544\n",
      "Step 400, total_loss: 1.193058729171753\n",
      "Step 440, total_loss: 1.043522834777832\n",
      "Step 480, total_loss: 1.0972378253936768\n",
      "Step 520, total_loss: 0.9340924024581909\n",
      "Step 560, total_loss: 1.0643209218978882\n",
      "Step 600, total_loss: 0.9257394075393677\n",
      "Step 640, total_loss: 0.5943058729171753\n",
      "Step 680, total_loss: 0.6407249569892883\n",
      "Step 720, total_loss: 0.5495839715003967\n",
      "Step 760, total_loss: 0.4921736717224121\n",
      "Step 800, total_loss: 0.5572090148925781\n",
      "Step 840, total_loss: 0.5156943202018738\n",
      "Step 880, total_loss: 0.5976853966712952\n",
      "Step 920, total_loss: 0.8885796070098877\n",
      "Step 960, total_loss: 0.563235342502594\n",
      "Step 1000, total_loss: 0.45060306787490845\n",
      "Step 1040, total_loss: 0.32487794756889343\n",
      "Step 1080, total_loss: 0.21960142254829407\n",
      "Step 1120, total_loss: 0.19158059358596802\n",
      "Step 1160, total_loss: 0.31703266501426697\n",
      "Step 1200, total_loss: 0.5164874196052551\n",
      "Step 1240, total_loss: 0.6336120963096619\n",
      "Step 1280, total_loss: 0.4465310573577881\n",
      "Step 1320, total_loss: 0.337449848651886\n",
      "Step 1360, total_loss: 0.2958798110485077\n",
      "Step 1400, total_loss: 0.16051572561264038\n",
      "Step 1440, total_loss: 0.09998569637537003\n",
      "Step 1480, total_loss: 0.1685781329870224\n",
      "Step 1520, total_loss: 0.20746742188930511\n",
      "Step 1560, total_loss: 0.1574048548936844\n",
      "Step 1600, total_loss: 0.14510168135166168\n",
      "Step 1640, total_loss: 0.14653518795967102\n",
      "Step 1680, total_loss: 0.2641645073890686\n",
      "Step 1720, total_loss: 0.2568228840827942\n",
      "Step 1760, total_loss: 0.277452677488327\n",
      "Step 1800, total_loss: 0.1336299180984497\n",
      "Step 1840, total_loss: 0.07721937447786331\n",
      "Step 1880, total_loss: 0.06470490247011185\n",
      "Step 1920, total_loss: 0.03983915224671364\n",
      "Step 1960, total_loss: 0.07112056761980057\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "for step in range(2000):\n",
    "    # 生成数据\n",
    "    src, tgt, tgt_y, n_tokens = generate_random_batch(batch_size=2, max_length=max_length)\n",
    "\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 进行transformer的计算\n",
    "    out = model(src, tgt)\n",
    "    # 将结果送给最后的线性层进行预测\n",
    "    out = model.predictor(out)\n",
    "    \"\"\"\n",
    "    计算损失。由于训练时我们的是对所有的输出都进行预测，所以需要对out进行reshape一下。\n",
    "            我们的out的Shape为(batch_size, 词数, 词典大小)，view之后变为：\n",
    "            (batch_size*词数, 词典大小)。\n",
    "            而在这些预测结果中，我们只需要对非<pad>部分进行，所以需要进行正则化。也就是\n",
    "            除以n_tokens。\n",
    "    \"\"\"\n",
    "    loss = criteria(out.contiguous().view(-1, out.size(-1)), tgt_y.contiguous().view(-1)) / n_tokens\n",
    "    # 计算梯度\n",
    "    loss.backward()\n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss\n",
    "\n",
    "    # 每40次打印一下loss\n",
    "    if step != 0 and step % 40 == 0:\n",
    "        print(\"Step {}, total_loss: {}\".format(step, total_loss))\n",
    "        total_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c8a665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "# 随便定义一个src\n",
    "src = torch.LongTensor([[0, 5, 3, 4, 6, 8, 9, 9, 8, 1, 2, 2]])\n",
    "# tgt从<bos>开始，看看能不能重新输出src中的值\n",
    "tgt = torch.LongTensor([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1bcadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 5, 3, 4, 6, 8, 9, 9, 8, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "for i in range(max_length):\n",
    "    # 进行transformer计算\n",
    "    out = model(src, tgt)\n",
    "    # 预测结果，因为只需要看最后一个词，所以取`out[:, -1]`\n",
    "    predict = model.predictor(out[:, -1])\n",
    "    # 找出最大值的index\n",
    "    y = torch.argmax(predict, dim=1)\n",
    "    # 和之前的预测结果拼接到一起\n",
    "    tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)\n",
    "\n",
    "    # 如果为<eos>，说明预测结束，跳出循环\n",
    "    if y == 1:\n",
    "        break\n",
    "print(tgt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
